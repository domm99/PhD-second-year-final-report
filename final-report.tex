% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\usepackage[inline]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{a4paper, left=25mm, right=25mm, top=25mm, bottom=25mm}

% \usepackage{fontspec}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%


\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\scshape\Large PhD Programme in Computer Science and Engineering \par}
    \vspace{0.5cm}
    {\scshape\large Cycle XXXIX \par}
    \vspace{0.5cm}

    \rule{\linewidth}{0.4mm} \\ [0.1mm]
    \raisebox{0.2cm}{\rule{\linewidth}{0.8mm}} \\[0.8cm]
    {\huge\bfseries PhD Second Year -- Final Report \par}
    \vspace{0.8cm}
    \rule{\linewidth}{0.8mm} \\ [0.1pt]
    \raisebox{0.2cm}{\rule{\linewidth}{0.4mm}} \\[1.5cm]
    
    % {\Large PhD First Year -- Final Report \par}

    \vspace{1.5cm}
    
    \noindent
    \begin{minipage}[t]{0.45\textwidth}
        \raggedright
        \textbf{Commission:}\\[0.5cm]
        Prof. Mirko Viroli\\
        Prof. Danilo Pianini\\
        Prof. Matteo Ferrara
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \raggedleft
        \textbf{PhD Student:}\\[0.5cm]
        Davide Domini
    \end{minipage}
    
    \vfill
    
\end{titlepage}

\section{Research Context}\label{sec:intro}

\subsection{Context}
Computing devices have become ubiquitous in everyday life.
%
This trend has paved the way for research fields aimed at exploiting
 the potential of device collectives to build next-generation systems, 
 including: collective computing~\cite{DBLP:journals/computer/Abowd16}
 and Collective Adaptive Systems (CAS)~\cite{DBLP:journals/sttt/WirsingJN23,robyphdthesis}.
%
More in detail, following Mitchell's definition~\cite{DBLP:conf/metacognition/Mitchell05}, 
 in this thesis we refer to CAS as distributed systems comprising multiple agents 
 such that each agent:
 \begin{enumerate*}[label=(\roman*)]
	\item can interact with other agents either directly or indirectly;
	\item does not individually posses system-wide knowledge;
	\item can exhibit learning to expand its personal knowledge; and
	\item can make decisions based on collective or aggregated knowledge from some of its peers.
 \end{enumerate*}
%
Notably, we focus on systems involving a large number of agents -- potentially in the hundreds
 or thousands -- which is commonly referred to in the literature 
 as many-agents~\cite{DBLP:phd/ethos/Yang21a}.

\subsection{Opportunities and challenges}
These systems enable the development of innovative applications in a wide range of real-world domains, such as: 
 smart cities~\cite{DBLP:conf/icse/IftikharRBW017}, 
 traffic control~\cite{DBLP:journals/tits/ChuWCL20,DBLP:books/sp/Muller2011/ProthmannTBHMS11} 
 with autonomous vehicles~\cite{DBLP:journals/corr/BojarskiTDFFGJM16}, 
 coordinated robot swarms for search and rescue~\cite{DBLP:journals/ijon/ZhouLLXS21} 
 or environmental monitoring~\cite{DBLP:conf/acsos/AguzziVE23}, and many more.
%
Nevertheless, while these systems have significant potential, their engineering 
 presents several challenges.
%
First and foremost, control and decision-making demand particular attention.
%
Achieving an optimal balance between centralized and decentralized control is crucial, 
 as neither extreme is feasible nor desirable~\cite{DBLP:conf/coordination/CasadeiPVN19,DBLP:journals/tits/ChuWCL20,DBLP:journals/jair/LyuBXDA23} 
 in many dynamic systems. 
% 
Indeed, excessive centralization may lead to bottlenecks and single points of failure, 
 whereas complete decentralization can hinder coordination and consistency.
%
Additionally, the dynamic nature of these systems, characterized by constant 
 environmental changes, mobility, and potential component failures, requires adaptive 
 learning mechanisms capable of responding swiftly 
 to evolving conditions~\cite{DBLP:journals/swarm/PrasetyoMF19}.
%
Another key consideration is the locality principle, where operational efficiency and cost 
 are heavily influenced by the spatial proximity of data sources, processing units, and users.
%
Furthermore, partial observability introduces uncertainty, as individual components may have 
 limited or incomplete information about the global state, complicating accurate 
 decision-making~\cite{DBLP:conf/uai/HeDB22}.
%
Data privacy is also a growing concern, particularly in light of stringent regulations like GDPR~\cite{GDPR}
 in the European Union, necessitating privacy-preserving learning techniques.
%
Finally, data heterogeneity~\cite{DBLP:journals/fgcs/MaZLCQ22,DBLP:journals/ijon/ZhuXLJ21},
 stemming from diverse source, can significantly impact learning stability and accuracy.
%
Addressing these challenges is essential for the effective deployment of cooperative learning 
 in collective adaptive systems.

\section{Methodology and Preliminary Results}\label{sec:methodology}
Building upon the work conducted during the first year, 
 several activities have been carried forward and expanded.

First of all, the two approaches for field-based Federated Learning that we proposed and presented 
 at conferences during the first year have been further developed, consolidated, extensively tested, 
 and submitted for journal publication. 
 
In particular, the work~\cite{DBLP:conf/coordination/DominiAEV24}
 has been submitted to the journal Logical Methods in Computer Science, 
 while~\cite{DBLP:conf/acsos/DominiFAVE24} has been submitted to the special issue
 Collective Intelligence for the Internet of Things of the journal Internet of Things (Elsevier). 
% 
Both submissions have successfully passed the first round of reviews.

In addition, we initiated a new collaboration with the Free University of Bozen-Bolzano 
 and Radboud University Nijmegen. 
%
This work aims to integrate our self-organizing FL approach with sparse neural networks. 
%
The combination of these techniques enables us to significantly reduce training and inference times and costs, 
 thus making our approach applicable to smart city scenarios where devices have limited computational resources. 
%
This collaboration has already led to a paper accepted at the Workshop on Green Federated Learning 
 co-located with the International Joint Conference on Neural Networks 2025 (IJCNN 2025)~\cite{DBLP:journals/corr/abs-2507-07613}.

Finally, we developed a benchmark to validate approaches designed for scenarios similar to ours, 
 i.e., involving proximity-based non-iid data where device location influences data distribution. 
% 
The benchmark, called ProFed, is open-source and aims to facilitate the evaluation and comparison 
 of FL solutions under realistic spatially correlated data settings. 
% 
This work, carried out in collaboration with Aarhus University (Denmark), 
 has been submitted for publication to the Q1 Journal of Open Research Software (JORS).

\section{Future Work}\label{sec:future}

The investigation performed in this first two years, opened several research directions
 to be explored in the future.

\paragraph{Real-world use cases.}

First and foremost, it will be crucial to define a detailed set of real-world use cases where our proposed approaches can be applied. 
%
Potential applications include traffic control or heating management for smart buildings. 
%
This will allow us to test our methods not only on synthetic datasets but also in real-world scenarios, 
 ensuring their practical relevance and robustness. 
% 
Moreover, it will provide valuable insights into how the creation of specialized learning subregions, based on data distribution, 
 affects the size and complexity of the required models. 
% 
Our intuition is that decomposing a phenomenon into simpler subproblems will enable learning with smaller, more efficient models, 
 ultimately leading to resource savings.

\paragraph{Energy Efficiency and Sparse Neural Networks.}
Another key challenge in these systems is that devices often operate under strict computational constraints. 
%
For this reason, we see sparse neural networks as a promising research avenue. 
%
This technique enables the removal of unimportant weights in neural networks, allowing learning even on devices without 
 GPUs or powerful CPUs. 
% 
We have already begun experiments in this direction in collaboration with the University of Bolzano, 
 and our preliminary results are highly encouraging. 
% 
Future work will also include comparison with other model optimization techniques, such as quantization, 
 to further reduce the training footprint.

\paragraph{Neighbor-based MARL and Transfer Learning.}
Additional promising research directions stem from our work on neighbor-based MARL approaches.
%
Specifically, it will be interesting to investigate whether and how these methods stabilize learning in environments 
 where agents interact with highly localized phenomena. 
% 
Furthermore, in collaboration with a research group at Trinity College Dublin, we plan to explore the integration of neighbor-based 
 learning with transfer learning for reinforcement learning and aggregate computing.
%
This integration could provide multiple benefits. 
%
First, information exchange between agents would occur only when uncertainty is high in a given state, reducing unnecessary communication 
 and potentially improving training efficiency. 
% 
This way, an agent with low confidence in a state would avoid sharing misleading information with a more experienced agent, 
 which could otherwise negatively impact future learning.
%
Finally, by incorporating the self-organizing coordination region concept from aggregate computing, we can envision a scenario where 
 each region dynamically elects a teacher (i.e., the leader).
% 
This leader, having already learned the dynamics of its area, can assist nearby agents by sharing its experience. 
%
This could be particularly beneficial in settings where agents have heterogeneous models due to varying computational constraints. 
%
In such cases, a more powerful agent could train using a larger model, converge faster, and then act as a teacher to support resource-limited 
 agents in the learning process.

\paragraph{Continual Learning}
While our primary focus is on the more immediate research directions, 
 we also have a long-term vision for our work. 
% 
One possible future direction is the integration of continual learning (CL) techniques into our proposed approaches. 
%
Given that CAS are inherently dynamic and evolve over time, ensuring that the learning process can adapt to these changes is crucial. 
%
CL offers a promising solution by enabling models to learn from new data while retaining knowledge from past experiences. 
%
This would allow our models to remain up-to-date and effectively handle new scenarios without forgetting previously 
 acquired information.


\section{Period Abroad}
As part of my PhD program, I am currently carrying out a three-month research stay abroad, 
 which started in September 2025 and spans the transition between 
 the second and third year of my doctoral studies.

This research period is hosted at Trinity College Dublin (Ireland) 
 under the supervision of Prof. Ivana Dusparic. 
% 
The focus of this research stay is on the development and evaluation of neighbor-based transfer 
 learning techniques for multi-agent reinforcement learning (MARL).
%
The goal of this work is to design and analyze methods that enable agents to exploit information 
 from their neighboring peers to accelerate learning and improve coordination performance 
 in distributed MARL settings. 
% 
This line of research is complementary to my work on field-based federated learning and aims 
 to explore synergies between FL and MARL in scenarios characterized by spatially correlated and dynamic environments.

\section{Pubblications}
List of published (or already accepted) papers.

\noindent
First year:
\begin{enumerate}
    \item \emph{ScaRLib: A Framework for Cooperative Many Agent Deep Reinforcement Learning in Scala}~\cite{DBLP:conf/coordination/DominiCAV23} \\
    \textbf{Abstract: }
    Multi Agent Reinforcement Learning (MARL) is an emerging field in machine learning where multiple agents learn, simultaneously and in a shared environment, 
     how to optimise a global or local reward signal. MARL has gained significant interest in recent years due to its successful applications in various domains,
     such as robotics, IoT, and traffic control. Cooperative Many Agent Reinforcement Learning (CMARL) is a relevant subclass of MARL, where thousands of 
     agents work together to achieve a common coordination goal.
    %
    In this paper, we introduce ScaRLib, a Scala framework relying on state-of-the-art deep learning libraries to support the development of CMARL systems. 
    %
    The framework supports the specification of centralised training and decentralised execution, and it is designed to be easily extensible, allowing to add new 
     algorithms, new types of environments, and new coordination toolchains.
    %
    This paper describes the main structure and features of ScaRLib and includes basic demonstrations that showcase binding with one such toolchain: 
     ScaFi programming framework and Alchemist simulator can be exploited to enable learning of field-based coordination policies for large-scale systems.
    %
    \item \emph{Field-Based Coordination for Federated Learning}~\cite{DBLP:conf/coordination/DominiAEV24} \\
    \textbf{Abstract: }
    Federated Learning has gained increasing interest in the last years, as it allows the training of machine learning models with a large number of devices 
     by exchanging only the weights of the trained neural networks. 
    % 
    Without the need to upload the training data to a central server, privacy concerns and potential bottlenecks can be removed as fewer data is transmitted. 
    %
    However, the current state-of-the-art solutions are typically centralized, and do not provide for suitable coordination mechanisms to take into account 
     spatial distribution of devices and local communications, which can sometimes play a crucial role. 
    % 
    Therefore, we propose a field-based coordination approach for federated learning, where the devices coordinate with each other through the use of 
     computational fields. 
    %
    We show that this approach can be used to train models in a completely peer-to-peer fashion. 
    %
    Additionally, our approach also allows for emergently create zones of interests, and produce specialized models for each zone enabling each agent 
     to refine their models for the tasks at hand.
    %
    We evaluate our approach in a simulated environment leveraging aggregate computing—the reference global-to-local field-based coordination programming paradigm. 
    %
    The results show that our approach is comparable to the state-of-the-art centralized solutions, while enabling a more flexible and scalable approach 
     to federated learning.
    \item \emph{ScaRLib: Towards a hybrid toolchain for aggregate computing and many-agent reinforcement learning}~\cite{DBLP:journals/scp/DominiCAV24} \\
    \textbf{Abstract: }
    This article introduces ScaRLib, a Scala-based framework that aims to streamline the development cyber-physical swarms scenarios 
     (i.e., systems of many interacting distributed devices that collectively accomplish system-wide tasks) by integrating macroprogramming and multi-agent 
     reinforcement learning to design collective behavior. 
    %
     This framework serves as the starting point for a broader toolchain that will integrate these two approaches at multiple points to harness the 
     capabilities of both, enabling the expression of complex and adaptive collective behavior.

    \item \emph{Towards Intelligent Pulverized Systems: a Modern Approach for Edge-Cloud Services}~\cite{DBLP:conf/woa/DominiFAV24} \\
    \textbf{Abstract: }
    Emerging trends are leveraging the potential of the edge-cloud continuum to foster the creation of smart services capable of adapting to the 
     dynamic nature of modern computing landscapes. 
    %
     This adaptation is achievable through two primary methods: by leveraging the underlying architecture to refine machine learning algorithms, 
      and by implementing machine learning algorithms to optimize the distribution of resources and services intelligently. 
    %  
    This paper explores the latter approach, focusing on recent advancements in pulverized architecture, collective intelligence, and many-agent 
     reinforcement learning systems. 
    %
    This novel trend, which we refer to as intelligent pulverized system (IPS), aims to create a new generation of services that can adapt to the 
     complex and dynamic nature of the edge-cloud continuum. Our proposed learning framework integrates many-agent reinforcement learning, graph neural 
     networks, and aggregate computing to create intelligent services tailored for this environment. 
    %
    We discuss the application of this framework across different levels of the pulverization model, illustrating its potential to enhance 
     the adaptability and efficiency of services within the edge-cloud continuum.
    \item \emph{Proximity-based Self-Federated Learning}~\cite{DBLP:conf/acsos/DominiFAVE24} \\
    \textbf{Abstract: }
    In recent advancements in machine learning, federated learning allows a network of distributed clients to collaboratively develop a global model 
     without needing to share their local data. 
    %
    This technique aims to safeguard privacy, countering the vulnerabilities of conventional centralized learning methods. 
    %
    Traditional federated learning approaches often rely on a central server to coordinate model training across clients, aiming to replicate 
     the same model uniformly across all nodes. 
    %
    However, these methods overlook the significance of geographical and local data variances in vast networks, potentially affecting model 
     effectiveness and applicability. 
    % 
    Moreover, relying on a central server might become a bottleneck in large networks, such as the ones promoted by edge computing. 
    %
    Our paper introduces a novel, fully-distributed federated learning strategy called proximity-based self-federated learning that enables 
     the self-organised creation of multiple federations of clients based on their geographic proximity and data distribution without 
     exchanging raw data.
    %
    Indeed, unlike traditional algorithms, our approach encourages clients to share and adjust their models with neighbouring nodes based on geographic 
     proximity and model accuracy. 
    %
    This method not only addresses the limitations posed by diverse data distributions but also enhances the model's adaptability to different regional 
     characteristics creating specialized models for each federation. We demonstrate the efficacy of our approach through simulations on 
     well-known datasets, showcasing its effectiveness over the conventional centralized federated learning framework.

    \item \emph{Towards Self-Adaptive Cooperative Learning in Collective Systems}~\cite{DBLP:conf/acsos/Domini24} \\
    \textbf{Abstract: }
    Nowadays, collective adaptive systems have become crucial as modern systems increasingly adopt this vision.
    %
    These systems can be leveraged as a means to facilitate cooperative adaptive learning.
    %
    However, implementing such systems presents various challenges, including:
     scalability, failures, non-iid data and complex architectures.
    %
    This paper presents a modern approach to cooperative and privacy-resilient learning by leveraging macroprogramming.
    %
    Specifically, we propose a new framework based on the integration of aggregate computing and federated learning,
     aiming to address these challenges and enhance the effectiveness and security of cooperative learning systems.

    \item \emph{A Reusable Simulation Pipeline for Many-Agent Reinforcement Learning}~\cite{DBLP:conf/dsrt/DominiAPV24} \\
    \textbf{Abstract: }
    Recent advancements in multi-agent reinforcement learning led to systems in which large groups of agents
     work together to learn shared policies and achieve collective behavior.
    %
    This approach is increasingly important for many applications,
     including swarm robotics, crowd sensing, and large-scale IoT networks.
    %
    In fact, these systems require repeated experimentation to learn from experience:
     simulation becomes thus essential, as deploying and testing in real-world environments incurs in high costs and practical challenges.
    %
    In response to this need, our paper introduces a simulation-based pipeline to gather the necessary experience for many-agent learning.
    %
    We highlight the requirements of such pipeline and the role of simulation, presenting also a practical prototype implemented in Alchemist,
     a simulator designed for very large-scale systems.
    %
    This pipeline provides a scalable, modular, and flexible environment for developing and testing many-agent reinforcement learning strategies.
\end{enumerate}

\noindent
Second year:
\begin{enumerate}
    \item \emph{FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning}~\cite{DBLP:journals/corr/abs-2502-08577} (Submitted to Logical Method in Computer Science)\\
    \textbf{Abstract: }
    In the last years, Federated Learning (FL) has become a popular solution to train machine learning models
     in domains with high privacy concerns.
    %
    However, 
     FL scalability and performance face significant challenges 
     in real-world deployments where data across devices are non-independently and identically 
     distributed (non-IID).
    %
    The heterogeneity in data distribution frequently arises from spatial distribution of devices,
     leading to degraded model performance in the absence of proper handling.
    %
    Additionally, 
     FL typical reliance on centralized architectures introduces bottlenecks
     and single-point-of-failure risks, particularly problematic at scale or in dynamic environments.

    To close this gap, 
     we propose FBFL, a novel approach leveraging 
     macroprogramming and field coordination to address these limitations through:
    \begin{enumerate*}[label=(\roman*)]
        \item distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and
        \item construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. 
    \end{enumerate*}
    %
    Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development 
     of more specialized models tailored to the specific data distribution in each subregion.

    %
    This paper formalizes FBFL and evaluates it extensively using 
     MNIST, FashionMNIST, and Extended MNIST datasets. 
    %
    We demonstrate that, when operating under IID data conditions, FBFL performs 
     comparably to the widely-used FedAvg algorithm.
    %
    Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also 
     surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been 
     specifically designed to address non-IID data distributions.
    %
    Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.

    \item \emph{Neighbor-Based Decentralized Training Strategies for Multi-Agent Reinforcement Learning}~\cite{DBLP:conf/sac/MalucelliDAV25} \\
    \textbf{Abstract: }
    Multi-agent deep reinforcement learning has demonstrated significant potential as a promising framework for developing 
     autonomous agents capable of operating within complex, multi-agent environments  
     in a wide range of domains, like robotics, traffic management, and video games.
    %
    Centralized training with decentralized execution has emerged as the predominant training paradigm, 
     demonstrating significant effectiveness in learning complex policies.
    %
    However, its reliance on a centralized learner necessitates that agents acquire policies offline and subsequently 
     execute them online.
    % 
    This constraint motivates the exploration of decentralized training methodologies.
    %
    Despite their greater flexibility, decentralized approaches often face critical challenges, like: 
     slower convergence rates, higher instability and lower performance compared to centralized methods. 
    %
    Therefore, this paper proposes three neighbor-based decentralized training 
     strategies based on the Deep-Q Learning algorithm and investigates their effectiveness as 
     a viable alternative to centralized training. 
    %  
    We evaluate experience sharing, k-nearest neighbor averaging, and k-nearest neighbor consensus methods 
     in a cooperative multi-agent environment and compare their performance against centralized training
     and totally decentralized training. 
    %  
    Our results show that neighbor-based methods can achieve comparable performance to centralized training 
     while offering improved scalability and communication efficiency.
    
    \item \emph{Sparse Self-Federated Learning for Energy Efficient Cooperative Intelligence in Society 5.0}~\cite{DBLP:journals/corr/abs-2507-07613} 
     (Accepted at the Green Federated Learning Workshop 2025 ) \\
    \textbf{Abstract: }
    Federated Learning offers privacy-preserving collaborative intelligence but struggles to meet the sustainability demands 
     of emerging IoT ecosystems necessary for Society 5.0---a human-centered technological future balancing social advancement 
     with environmental responsibility. 
    % 
    The excessive communication bandwidth and computational resources required by traditional FL approaches make them 
     environmentally unsustainable at scale, creating a fundamental conflict with green AI principles as billions 
     of resource-constrained devices attempt to participate. 
    % 
    To this end, we introduce Sparse Proximity-based Self-Federated Learning (SParSeFuL), 
     a resource-aware approach that bridges this gap by combining aggregate computing for self-organization with neural network 
     sparsification to reduce energy and bandwidth consumption.
     
    \item \emph{A Demonstrator for Self-organizing Robot Teams}~\cite{DBLP:conf/coordination/AguzziBBCCDFPV25} \\
    \textbf{Abstract: }
    Aggregate computing is a paradigm with over a decade of investigation and multiple programming frameworks available, 
     which proved to be particularly suitable for the simulation of applications in challenging domains such as smart cities
     and robot swarms. 
    % 
    This paper introduces a toolchain for practical multi-robot demonstrations based on aggregate computing principles, 
     and validates it with a live interactive demo in an open-public event in the context of the European Researchers' Night. 
    % 
    More specifically, we show how we coordinated a team of mobile robots to form spatial patterns.
    %
    We discuss the practical demonstration performed in an indoor environment, 
     which exploits a camera system and ArUco markers for localization.
    \item \emph{Decentralized Proximity-Aware Clustering for Collective Self-Federated Learning} \\ (Submitted to Elsevier Internet of Things Journal) \\
    \textbf{Abstract: }
    In recent years, 
     Federated Learning (FL) has emerged as a privacy-preserving paradigm for collaborative model training in IoT systems, 
     enabling clients to learn a global model for tasks like classification, prediction, 
     or anomaly detection in IoT environments without sharing raw data.
    %
    However, 
     traditional centralized FL architectures face bottlenecks, single points of failure, 
     and struggle with non-IID data.
    %
    These limitations hinder effective Collective Intelligence in large-scale IoT systems 
     where numerous devices operate across diverse and dynamic environments.
    % 
    Existing clustered FL approaches often retain centralization or 
     overlook how the spatial distribution inherent in IoT deployments 
     directly influences data heterogeneity, 
     challenging both the integration of spatially correlated devices and the
     establishment of intelligence distributed across the entire system.
    %
    Creating such intelligence demands both decentralized architectures for scalability 
     and effective integration of devices with similar data distributions.

    For these reasons, this article introduces \emph{Proximity-Aware Self-Federated Learning (PSFL)}, 
     a novel decentralized approach embodying collective intelligence principles. 
    %
    PSFL leverages field-based coordination to enable IoT devices to form \emph{self-federations},
     dynamically clustered groups that train specialized models based on both spatial proximity and local model characteristics. 
     These self-federations reflect underlying data distributions, 
     creating a distributed ecosystem of specialized models across the network.
    %
    This approach overcomes global model limitations in non-IID settings through 
     specialized federations based on local data distributions, enhancing performance
     while maintaining decentralization.
    %
    We evaluate our approach using the Extended MNIST and CIFAR-100 datasets against state-of-the-art baselines,
     demonstrating its effectiveness in forming coherent, 
     localized models under non-IID conditions.
    \item \emph{Heterogeneous GNN for Collective-Task Offloading in Cloud-Edge via Deep Q-Learning} \\ (Submitted to Future Generation Computer Systems Journal) \\
    \textbf{Abstract: }
    Task offloading in edge-cloud computing systems requires determining optimal allocation of 
     application components across heterogeneous infrastructure while balancing multiple objectives, 
     like energy consumption, latency, or cost.
    %
    This problem becomes particularly complex in large-scale deployments (e.g., smart cities, industrial IoT) 
     where existing approaches fail to address collective phenomena, 
     namely emergent system-wide behaviors like network congestion that arise from multi-device interactions, 
     leading to suboptimal offloading decisions in large-scale deployments.
    %
    To address these challenges 
     this paper introduces a multi-agent learning framework for collective component offloading 
     that decomposes applications into a directed acyclic graph of macro-components, 
     enabling partial offloading where individual components can be selectively executed locally or migrated to edge/cloud servers.
    %
    Our system model represents the infrastructure as a heterogeneous graph of application devices 
     and infrastructure nodes, 
     supporting decentralized offloading decisions while maintaining component interdependencies.
     In particular, we propose Informed Deep Hetero Graph Q-Learning (IDHGQL), 
     \begin{enumerate*}[label=(\arabic*)]
      \item Heterogeneous Graph Neural Network for policy representation that naturally handle diverse device types and relationships.
      \item Aggregate computing to enrich device observations with collective system state information.
      \item A multi-agent Deep Q-Learning algorithm based on centralized training with decentralized execution that balances individual constraints with emergent collective phenomena.
     \end{enumerate*}
    %
    Experimental evaluation demonstrates IDHGQL's effectiveness in multi-objective optimization scenarios,
     successfully learning policies that balance battery consumption, 
     latency, and infrastructure costs. 
    %
    In density-aware scenarios, 
     agents learn spatially-differentiated strategies, 
     namely executing locally in high-congestion areas while offloading in sparse regions, 
     achieving superior performance compared to static baselines 
     and validating the critical importance of collective information integration.
    \item \emph{ProFed: a Benchmark for Proximity-based non-IID Federated Learning} \\ (Submitted to Journal of Open Research Software) \\
    \textbf{Abstract: }
    Federated Learning has emerged as a key paradigm in machine learning, 
     but its performance often deteriorates under non-independent and identically distributed 
     (non-IID) client data. 
    % 
    Such heterogeneity frequently reflects geographic factors, e.g., regional linguistic variations 
     or localized traffic patterns, leading to IID data within regions but 
     non-IID distributions across them. 
    %
    However, existing FL algorithms are typically evaluated by randomly splitting 
     non-IID data across devices, disregarding their spatial distribution.
        
    To address this gap, we introduce \textsc{ProFed}, a benchmark that simulates data splits 
     with varying degrees of skewness across different regions. 
    %
    We incorporate several skewness methods from the literature and apply them to 
     well-known datasets, including MNIST, FashionMNIST, CIFAR-10, CIFAR-100 and UTKFace.
    %
    Our goal is to provide researchers with a standardized framework to evaluate FL algorithms 
     more effectively and consistently against established baselines.
    \item \emph{Scaling Swarm Coordination with GNNs -- How Far Can We Go?} \\ (Submitted to MDPI AI) \\
    \textbf{Abstract: }
    The scalability of coordination policies is a critical challenge in swarm robotics, 
     where agent numbers may vary substantially between deployment scenarios. 
    %
    Graph Neural Networks (GNNs) have emerged as a promising tool for learning 
     decentralized policies from local interactions, 
     but their zero-shot transfer across swarm sizes remains underexplored. 
    % 
    We empirically study this by combining GNNs with Deep Q-Learning in cooperative swarms.
    %
    We formalize Deep Graph Q-Learning (DGQL), which processes agent-neighbor graphs within Q-learning, 
     and train on fixed-size swarms. Across two benchmarks (goal-reaching, obstacle avoidance), 
     we deploy up to $3\times$ larger teams. 
    % 
    DGQL preserves functional coordination without retraining, but efficiency degrades with size: 
     terminal goal distance grows monotonically (15-29 agents) and worsens beyond roughly $2\times$ the training size ($\approx 20$ agents), 
     with task-dependent trade-offs. 
    % 
    Our results quantify scalability limits of GNN-enhanced DQL and suggest architectural and training strategies 
     to better sustain performance across scales.
    \item \emph{SParSeFuL: Decentralized Federated Learning for Non-IID IoT Systems} \\ (Submitted to Annual AAAI Conference on Artificial Intelligence) \\
    \textbf{Abstract: }
    Federated learning has emerged as a promising solution for training machine learning models while preserving privacy in distributed environments.
    %
    However, deploying federated learning in large-scale Internet of Things (IoT) systems presents significant challenges: 
     data heterogeneity across spatially distributed devices, resource constraints of edge devices, and the lack of reliable centralized infrastructure.
    %
    Classical federated learning approaches struggle with non-IID data distributions and require a centralized server for model aggregation, 
     which leads to inefficiencies and high communication overhead.
    %
    To address these limitations, we propose Sparse Proximity-based Self-Federated Learning (SParSeFuL), 
     a novel decentralized architecture that combines self-organizing distributed systems with neural network 
     sparsification for sustainable and privacy-preserving machine learning at scale.
    %
    SParSeFuL leverages self-organizing methods to dynamically form federations of spatially proximate devices with similar data distributions, 
     enabling personalized model training that respects local data patterns while maintaining global learning objectives.
    %
    By integrating neural network sparsification techniques, our approach significantly reduces computational and communication overhead, 
     making it suitable for resource-constrained IoT environments.
    %
    To demonstrate the potential of SParSeFuL, we conduct an extensive experimental evaluation on the widely 
    used Extended MNIST dataset, comparing it against several state-of-the-art federated learning algorithms -- namely 
    FedAvg, FedProx, Scaffold, and IFCA -- showing that SParSeFuL outperforms most baselines even 
    at a sparsification level up to 70\%.

    
\end{enumerate}


\section{Attended Conferences and Workshops}
First year:
\begin{enumerate}
    \item \textbf{Coordination Models and Languages} - 26th International Conference, COORDINATION 2024, Held as Part of the 19th International Federated Conference on Distributed Computing Techniques, DisCoTec 2024, Groningen, The Netherlands, June 17-21, 2024
    \item \textbf{25th Workshop "From Objects to Agents"}, Bard (Aosta), Italy, July 8-10, 2024
    \item \textbf{IEEE International Conference on Autonomic Computing and Self-Organizing Systems}, ACSOS 2024, Aarhus, Denmark, September 16-20, 2024
    \item \textbf{11th Workshop on Self-Improving Systems Integration}, SISSY 2024, Held as Part of ACSOS 2024, Aarhus, Denmark, September 16-20, 2024
    \item \textbf{2nd International Workshop on Artificial Intelligence for Autonomous computing Systems}, AI4AS 2024, Held as Part of ACSOS 2024, Aarhus, Denmark, September 16-20, 2024
    \item \textbf{28th International Symposium on Distributed Simulation and Real Time Applications}, DS-RT 2024, Urbino, Italy, October 7-9, 2024
\end{enumerate}
Second year:
\begin{enumerate}
    \item \textbf{40th ACM/SIGAPP Symposium On Applied Computing}, SAC 2025, Catania, Italy, March 31 - April 4, 2025
    \item \textbf{International Joint Conference on Neural Networks}, IJCNN 2025, Rome, Italy,  June 30 - July 5, 2025
    \item \textbf{Workshop on Green Federated Learning}, GFL 2025, Rome, Italy,  June 30 - July 5, 2025
\end{enumerate}

\section{Doctoral Schools}
\begin{enumerate}
    \item \textbf{Bertionoro International Spring School 2024}, BISS 2024, Bertinoro, Italy, March 11-15, 2024
    \item \textbf{SpaceRaise 2025}, SpaseRaise 2025, L'Aquila, Italy, May 26-30, 2025
    \item \textbf{International Software Engineering Summer School 2025}, SIESTA 2025, Lugano, Switzerland, August 27-29, 2025
\end{enumerate}

\section{Teaching Tutor}
First Year:
\begin{enumerate}
    \item \textbf{Machine Learning Systems for Data Science} - Statistical Science Bachelor Degree
    \item \textbf{Software Engineering (Modulo 1)} - Digital Transformation Management Master Degree
\end{enumerate}
\noindent
Second Year:
\begin{enumerate}
    \item \textbf{Software Engineering} - Digital Transformation Management Master Degree
    \item \textbf{Fondamenti di Informatica} - Biomedical Engineering Bachelor Degree
\end{enumerate}

\section{PhD Courses}

\begin{enumerate}
    \item \textbf{BISS 2024 - Bertinoro International Spring School - Program Analysis}
    \begin{itemize}
        \item Prof: Roberta Gori, Roberto Bruni  - Università di Pisa
        \item Proposed CFU: 4 (12 hours) 
        \item Period: March 2024
        \item Exam: Not done, CFUs without evaluation
    \end{itemize}    
    \item \textbf{BISS 2024 - Bertinoro International Spring School - Graph Neural Networks}
    \begin{itemize}
        \item Prof: Fabrizio Silvestri - Sapienza Università di Roma
        \item Proposed CFU: 4 (12 hours) 
        \item Period: March 2024
        \item Exam: Done
    \end{itemize}  
    \item \textbf{BISS 2024 - Bertinoro International Spring School - Large Language Models}
    \begin{itemize}
        \item Prof: Danilo Croce - University of Roma, Tor Vergata
        \item Proposed CFU: 4 (12 hours) 
        \item Period: March 2024
        \item Exam: Done
    \end{itemize}  
    \item \textbf{Risk Assessment of ML for Cybersecurity}
    \begin{itemize}
        \item Fabio Pierazzi - King's College London
        \item Proposed CFU: 5 (20 hours) 
        \item Period: April 2024
        \item Exam: Done
    \end{itemize}    
    \item \textbf{Introduction to Complex Systems Science} 
    \begin{itemize}
        \item Prof: Andrea Roli - Università di Bologna      
        \item Proposed CFU: 2 (10 hours) 
        \item Period: June 2024
        \item Exam: No
    \end{itemize}    
    \item \textbf{How to Write and Publish a Research Paper in Computer Science and Engineering} 
    \begin{itemize}
        \item Prof: Zeynep Kiziltan - Università di Bologna      
        \item Proposed CFU: 3 (12 hours) 
        \item Period: June 2025
        \item Exam: Done
    \end{itemize}  
    \item \textbf{Robust and Reproducible Experimental Deep Learning Setting} 
    \begin{itemize}
        \item Prof: Federico Ruggeri - Università di Bologna      
        \item Proposed CFU: 3 (16 hours) 
        \item Period: April-May 2025
        \item Exam: Done
    \end{itemize} 
    \item \textbf{Service Orchestration and Industrial IoT Platforms for Industry 4 and 5.0 environments} 
    \begin{itemize}
        \item Prof: Riccardo Venanzi - Università di Bologna      
        \item Proposed CFU: 2 (12 hours) 
        \item Period: January 2025
        \item Exam: Done
    \end{itemize} 
    \item \textbf{Data visualization for research} 
    \begin{itemize}
        \item Prof: Chiara Ceccarini - Università di Bologna      
        \item Proposed CFU: 2 (12 hours) 
        \item Period: May 2025
        \item Exam: Done
    \end{itemize} 
    \item \textbf{Multi-platform Programming for Research-Oriented Software} 
    \begin{itemize}
        \item Prof: Giovanni Ciatto - Università di Bologna      
        \item Proposed CFU: 3 (12 hours) 
        \item Period: November 2024
        \item Exam: No
    \end{itemize} 
    \item \textbf{Containerisation and Orchestration for Research Reproducibility} 
    \begin{itemize}
        \item Prof: Giovanni Ciatto - Università di Bologna      
        \item Proposed CFU: 2 (12 hours) 
        \item Period: October 2024
        \item Exam: No
    \end{itemize} 
    
\end{enumerate}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{alpha}
\bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
